{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a7307d-e273-463c-9594-50413d50198d",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "\n",
    "## Objective 1\n",
    "\n",
    "- Project performs news title analysis on all stocks listed in the NYSE and Nasdaq. It then produces a list of the top 100 long options and top 100 short options\n",
    "- Leave unweighted to take into account trending stocks\n",
    "- do sentiment analysis for sectors too, so as to reccomend sector etfs\n",
    "- add filter option for stocks such as sector and technical fundamentals, such as cap and pe\n",
    "- list  stocks with best sentiment by sector\n",
    "\n",
    "\n",
    "- Find pattern using previous news stories. use week 1 as base, and week as tuning. objective is to estimate week 2 ending value, or at least direction up/down\n",
    "\n",
    "- add scales to the keywords\n",
    "- translate to not waste headlines\n",
    "\n",
    "\n",
    "# THERE ARE REQUEST LIMITS IN THE API. 50 MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28f29c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source', 'author', 'title', 'description', 'url', 'urlToImage',\n",
      "       'publishedAt', 'content'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#old code ignore this cell\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "Stocklist=['XOM','NVDA','INTC','TSLA','F','T','KHC']\n",
    "url = ('https://newsapi.org/v2/everything?'\n",
    "       'q=XOM&'\n",
    "       'from=2024-07-5&'\n",
    "       'to=2024-06-5'\n",
    "       'sortBy=popularity&'\n",
    "       'apiKey=a7cd801675e34669b53c3e9f7cffc4a7')\n",
    "# Use stock ticker not company name\n",
    "#print(response.json())\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Need to store ticket and title\n",
    "data=response.json()\n",
    "#print(data.keys())\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data['articles'])\n",
    "print(df.columns)\n",
    "df=df.drop(columns=['source', 'author', 'description', 'url', 'urlToImage', 'publishedAt', 'content'])\n",
    "if 'Ticker' not in df.columns:\n",
    "    df.insert(0, 'Ticker', 'XOM')\n",
    "#df['Ticker']='XOM'\n",
    "df.to_csv('News.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dcbc366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.7292082225034218, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet \n",
    "\n",
    "# use the detect method to find the encoding\n",
    "# 'rb' means read in the file as binary\n",
    "with open('NYSELIST.csv', 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d63e44-0e56-4d89-8fad-2ec34f45e467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'articles'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(data\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m---> 29\u001b[0m     df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m  \u001b[38;5;66;03m#   df=df.drop(columns=['source', 'author', 'description', 'url', 'urlToImage', 'publishedAt', 'content'])\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m, i)   \n",
      "\u001b[1;31mKeyError\u001b[0m: 'articles'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "current_date = datetime.now().date()\n",
    "date_30_days_ago = current_date - timedelta(days=30)\n",
    "\n",
    "\n",
    "df_main=pd.DataFrame()\n",
    "file_path = 'NYSELIST.csv'\n",
    "Stocklist =  pd.read_csv(\"NYSELIST.csv\", encoding=\"Windows-1252\")\n",
    "Stocklist.drop(columns=[' Company'])\n",
    "#print(Stocklist[\" Company\"])\n",
    "Stocklist=['Exxon','Nvidia','Intel','Tesla','Ford','Kraft Heinz']\n",
    "#Stocklist[\" Symbol\"]\n",
    "for i in Stocklist:\n",
    "  #  print(i)\n",
    "    url = ('https://newsapi.org/v2/everything?'\n",
    "           f'q={i}&'\n",
    "           f'from=q{current_date}&'\n",
    "           f'to=q{date_30_days_ago}'\n",
    "           'sortBy=popularity&'\n",
    "           'apiKey=a7cd801675e34669b53c3e9f7cffc4a7')\n",
    "    response = requests.get(url)\n",
    "    data=response.json()\n",
    "    if(data!=None):\n",
    "        print(data)\n",
    "        df=pd.DataFrame(data['articles'])\n",
    "     #   df=df.drop(columns=['source', 'author', 'description', 'url', 'urlToImage', 'publishedAt', 'content'])\n",
    "        df.insert(0, 'Ticker', i)   \n",
    "        df_main = pd.concat([df_main, df])\n",
    "\n",
    "    #df_main.append(df, ignore_index=True)\n",
    "\n",
    "# Use stock ticker not company name\n",
    "#print(response.json())\n",
    "pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "def contains_non_ascii(row):\n",
    "    return any(pattern.search(str(value)) for value in row)\n",
    "df_main = df_main[~df_main.apply(contains_non_ascii, axis=1)]\n",
    "\n",
    "\n",
    "#def contains_ticker(row):\n",
    "#    return any(pattern.search(elem in for elem in Stocklist) for value in row)\n",
    "\n",
    "mask = df_main['title'].apply(lambda text: any(element in text for element in Stocklist))\n",
    "df_main=df_main[mask]\n",
    "\n",
    "df_main.to_csv('News.csv', index=False)\n",
    "\n",
    "# Need to store ticket and title\n",
    "\n",
    "#print(data.keys())\n",
    "\n",
    "\n",
    "\n",
    "#print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680a6ae7-1d75-43ad-81cb-6579edcec025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f480d3af-70ab-4a68-af2c-e8d7d6498fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Denali Advisors LLC Acquires Shares of 10,100 Exxon Mobil Co. (NYSE:XOM)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['articles'][0])['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80ecf5-f042-40da-a489-0d8380426cca",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "### Positive\n",
    "\n",
    "- Bought\n",
    "- Acquired\n",
    "- Grew\n",
    "- Raised\n",
    "- was up\n",
    "- Climbed\n",
    "\n",
    "### Negative\n",
    "\n",
    "- Trimmed\n",
    "- Sold\n",
    "- Cut\n",
    "- was down\n",
    "- Crashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700bb46-c32f-42dc-9c4f-af70d5e1bf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541a0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '|']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '|']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '&']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'News.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "SentimentValue=0\n",
    "\n",
    "# Define keywords for positive and negative sentiment\n",
    "\n",
    "#original arrays\n",
    "#positive_keywords = ['tops','jumps','purchase','increase','sign','giant','deal','bullish','growth', 'profit', 'success','safe','generate', 'win','won', 'gain', 'improve', 'raised', 'bought', 'buy', 'acquire','acquires', 'grew','gain','Rally', 'melt up', 'was up', 'up', 'climbed', 'Rally', 'Rallied']\n",
    "#negative_keywords = ['boasts','cancel','suspension','stall','quashed','bearish','loss', 'lost','recalls', 'decline','disaster', 'problem', 'lawsuit', 'fail', 'drop', 'trimmed', 'sell', 'sold', 'cut', 'was down', 'crash', 'decrying', 'melt down', 'broken']\n",
    "\n",
    "\n",
    "positive_keywords = ['tops','jumps','purchase','increase','sign','giant','deal','growth', 'profit', 'success','safe','generate', 'win','won', 'gain', 'improve', 'raised', 'bought', 'buy', 'acquire','acquires', 'grew','gain', 'was up', 'up', 'climbed']\n",
    "very_positive_keywords=['bullish', 'rally', 'melt up', 'rallied']\n",
    "negative_keywords = ['boasts','cancel','suspension','stall','quashed','loss', 'lost','recalls', 'decline','disaster', 'problem', 'lawsuit', 'fail', 'drop']\n",
    "very_negative_keywords=['bearish', 'trimmed', 'sell', 'sold', 'cut', 'was down', 'crash', 'decrying', 'melt down', 'broken']\n",
    "#the below doesnt work, first positive or negative word it sees, it marks , it ignores counters\n",
    "\n",
    "def rule_based_labeling(title):\n",
    "    title = title.lower()\n",
    "    title=title.split(\" \")\n",
    "    count_positive=0\n",
    "    count_negative=0\n",
    "    for word in title:\n",
    "        match, similarity=process.extractOne(word, positive_keywords)\n",
    "        if(similarity>=80):\n",
    "            count_positive+=1\n",
    "        match, similarity=process.extractOne(word, negative_keywords)\n",
    "        if(similarity>=80):\n",
    "            count_negative+=1            \n",
    "        match, similarity=process.extractOne(word, very_positive_keywords)\n",
    "        if(similarity>=80):\n",
    "            count_positive+=2\n",
    "        match, similarity=process.extractOne(word, very_negative_keywords)\n",
    "        if(similarity>=80):\n",
    "            count_negative+=2\n",
    "    return count_positive-count_negative\n",
    "        \n",
    "\n",
    "\n",
    "# Apply rule-based labeling\n",
    "df['sentiment'] = df['title'].apply(rule_based_labeling)\n",
    "\n",
    "output_file_path = 'news_with_sentiment.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900571ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
